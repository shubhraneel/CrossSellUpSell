{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import math\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('mt_dict.json',)\n",
    "mt_dict = json.load(f)\n",
    "mt_dict = {int(k):v for k, v in mt_dict.items()}\n",
    "\n",
    "\n",
    "f = open('wh_dict.json',)\n",
    "wh_dict = json.load(f)\n",
    "wh_dict = {int(k):v for k, v in wh_dict.items()}\n",
    "\n",
    "\n",
    "f = open('wh_to_group.json',)\n",
    "wh_to_group = json.load(f)\n",
    "wh_to_group = {int(k):v for k, v in wh_to_group.items()}\n",
    "\n",
    "\n",
    "f = open('wh_to_mid.json',)\n",
    "wh_to_mid = json.load(f)\n",
    "wh_to_mid = {int(k):v for k, v in wh_to_mid.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_HEADER = ['Wholesaler', 'material', 'hl_seq', 'group', 'mid']\n",
    "\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"Wholesaler\": list(wh_dict.values()),\n",
    "    \"material\": list(mt_dict.values()),\n",
    "    \"group\": list(np.unique(np.array(list(wh_to_group.values())))),\n",
    "    \"mid\": list(np.unique(np.array(list(wh_to_mid.values())))),\n",
    "#     \"occupation\": list(users.occupation.unique()),\n",
    "}\n",
    "\n",
    "USER_FEATURES = [\"group\", \"mid\"]\n",
    "\n",
    "MATERIAL_FEATURES = [\"deg_alc\"]\n",
    "\n",
    "#################\n",
    "sequence_length = 4\n",
    "step_size = 1\n",
    "def create_model_inputs():\n",
    "    return {\n",
    "        \"Wholesaler\": layers.Input(name=\"Wholesaler\", shape=(1,), dtype=tf.string),\n",
    "        \"material\": layers.Input(\n",
    "            name=\"material\", shape=(sequence_length - 1,), dtype=tf.string\n",
    "        ),\n",
    "        \"target_material\": layers.Input(\n",
    "            name=\"target_material\", shape=(1,), dtype=tf.string\n",
    "        ),\n",
    "        \"hl_seq\": layers.Input(\n",
    "            name=\"hl_seq\", shape=(sequence_length - 1,), dtype=tf.float32\n",
    "        ),\n",
    "        \"group\": layers.Input(name=\"group\", shape=(1,), dtype=tf.string),\n",
    "        \"mid\": layers.Input(name=\"mid\", shape=(1,), dtype=tf.string),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "############\n",
    "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
    "    def process(features):\n",
    "        material_string = features[\"material\"]\n",
    "        material = tf.strings.split(material_string, \",\").to_tensor()\n",
    "\n",
    "        # The last material id in the sequence is the target material.\n",
    "        features[\"target_material\"] = material[:, -1]\n",
    "        features[\"material\"] = material[:, :-1]\n",
    "\n",
    "        hl_seq_string = features[\"hl_seq\"]\n",
    "        hl_seq = tf.strings.to_number(\n",
    "            tf.strings.split(hl_seq_string, \",\"), tf.dtypes.float32\n",
    "        ).to_tensor()\n",
    "\n",
    "        # The last rating in the sequence is the target for the model to predict.\n",
    "        target = (hl_seq[:, -1])\n",
    "        features[\"hl_seq\"] = hl_seq[:, :-1]\n",
    "\n",
    "        return features, target\n",
    "\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        field_delim=\"|\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(process)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def encode_input_features(\n",
    "    inputs,\n",
    "    include_user_id=True,\n",
    "    include_user_features=True,\n",
    "    include_material_features=True,\n",
    "):\n",
    "\n",
    "    encoded_transformer_features = []\n",
    "    encoded_other_features = []\n",
    "\n",
    "    other_feature_names = []\n",
    "    if include_user_id:\n",
    "        other_feature_names.append(\"Wholesaler\")\n",
    "    if include_user_features:\n",
    "        other_feature_names.extend(USER_FEATURES)\n",
    "\n",
    "    ## Encode user features\n",
    "    for feature_name in other_feature_names:\n",
    "        # Convert the string input values into integer indices.\n",
    "        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "        idx = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)(\n",
    "            inputs[feature_name]\n",
    "        )\n",
    "        # Compute embedding dimensions\n",
    "        embedding_dims = int(math.sqrt(len(vocabulary)))\n",
    "        # Create an embedding layer with the specified dimensions.\n",
    "        embedding_encoder = layers.Embedding(\n",
    "            input_dim=len(vocabulary),\n",
    "            output_dim=embedding_dims,\n",
    "            name=f\"{feature_name}_embedding\",\n",
    "        )\n",
    "        # Convert the index values to embedding representations.\n",
    "        encoded_other_features.append(embedding_encoder(idx))\n",
    "\n",
    "    ## Create a single embedding vector for the user features\n",
    "    if len(encoded_other_features) > 1:\n",
    "        encoded_other_features = layers.concatenate(encoded_other_features)\n",
    "    elif len(encoded_other_features) == 1:\n",
    "        encoded_other_features = encoded_other_features[0]\n",
    "    else:\n",
    "        encoded_other_features = None\n",
    "\n",
    "    ## Create a material embedding encoder\n",
    "    material_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[\"material\"]\n",
    "    material_embedding_dims = int(math.sqrt(len(material_vocabulary)))\n",
    "    # Create a lookup to convert string values to integer indices.\n",
    "    material_index_lookup = StringLookup(\n",
    "        vocabulary=material_vocabulary,\n",
    "        mask_token=None,\n",
    "        num_oov_indices=0,\n",
    "        name=\"material_index_lookup\",\n",
    "    )\n",
    "    # Create an embedding layer with the specified dimensions.\n",
    "    material_embedding_encoder = layers.Embedding(\n",
    "        input_dim=len(material_vocabulary),\n",
    "        output_dim=material_embedding_dims,\n",
    "        name=f\"material_embedding\",\n",
    "    )\n",
    "#     # Create a vector lookup for material features.\n",
    "#     material_feature_vectors = deg_alc.to_numpy()\n",
    "#     material_feature_lookup = layers.Embedding(\n",
    "#         input_dim=material_feature_vectors.shape[0],\n",
    "#         output_dim=material_feature_vectors.shape[1],\n",
    "#         embeddings_initializer=tf.keras.initializers.Constant(material_feature_vectors),\n",
    "#         trainable=False,\n",
    "#         name=\"features_vector\",\n",
    "#     )\n",
    "#     # Create a processing layer for genres.\n",
    "#     material_embedding_processor = layers.Dense(\n",
    "#         units=material_embedding_dims,\n",
    "#         activation=\"relu\",\n",
    "#         name=\"process_material_embedding_with_genres\",\n",
    "#     )\n",
    "\n",
    "    ## Define a function to encode a given material id.\n",
    "    def encode_material(material_id):\n",
    "        # Convert the string input values into integer indices.\n",
    "        material_idx = material_index_lookup(material_id)\n",
    "        material_embedding = material_embedding_encoder(material_idx)\n",
    "        encoded_material = material_embedding\n",
    "        if include_material_features:\n",
    "            material_feature_vector = material_feature_lookup(material_idx)\n",
    "            encoded_material = material_embedding_processor(\n",
    "                layers.concatenate([material_embedding, material_feature_vector])\n",
    "            )\n",
    "        return encoded_material\n",
    "\n",
    "    ## Encoding target_material_id\n",
    "    target_material_id = inputs[\"target_material\"]\n",
    "    encoded_target_material = encode_material(target_material_id)\n",
    "\n",
    "    ## Encoding sequence material_ids.\n",
    "    sequence_material_ids = inputs[\"material\"]\n",
    "    encoded_sequence_material = encode_material(sequence_material_ids)\n",
    "    # Create positional embedding.\n",
    "    position_embedding_encoder = layers.Embedding(\n",
    "        input_dim=sequence_length,\n",
    "        output_dim=material_embedding_dims,\n",
    "        name=\"position_embedding\",\n",
    "    )\n",
    "    positions = tf.range(start=0, limit=sequence_length - 1, delta=1)\n",
    "    encodded_positions = position_embedding_encoder(positions)\n",
    "    # Retrieve sequence ratings to incorporate them into the encoding of the material.\n",
    "    sequence_ratings = tf.expand_dims(inputs[\"hl_seq\"], -1)\n",
    "    # Add the positional encoding to the material encodings and multiply them by rating.\n",
    "    encoded_sequence_material_with_poistion_and_rating = layers.Multiply()(\n",
    "        [(encoded_sequence_material + encodded_positions), sequence_ratings]\n",
    "    )\n",
    "\n",
    "    # Construct the transformer inputs.\n",
    "    for encoded_material in tf.unstack(\n",
    "        encoded_sequence_material_with_poistion_and_rating, axis=1\n",
    "    ):\n",
    "        encoded_transformer_features.append(tf.expand_dims(encoded_material, 1))\n",
    "    encoded_transformer_features.append(encoded_target_material)\n",
    "\n",
    "    encoded_transformer_features = layers.concatenate(\n",
    "        encoded_transformer_features, axis=1\n",
    "    )\n",
    "\n",
    "    return encoded_transformer_features, encoded_other_features\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "include_user_id = True\n",
    "include_user_features = True\n",
    "include_material_features = False\n",
    "\n",
    "hidden_units = [256, 128]\n",
    "dropout_rate = 0.1\n",
    "num_heads = 3\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    inputs = create_model_inputs()\n",
    "    transformer_features, other_features = encode_input_features(\n",
    "        inputs, include_user_id, include_user_features, include_material_features\n",
    "    )\n",
    "\n",
    "    # Create a multi-headed attention layer.\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=transformer_features.shape[2], dropout=dropout_rate\n",
    "    )(transformer_features, transformer_features)\n",
    "\n",
    "    # Transformer block.\n",
    "    attention_output = layers.Dropout(dropout_rate)(attention_output)\n",
    "    x1 = layers.Add()([transformer_features, attention_output])\n",
    "    x1 = layers.LayerNormalization()(x1)\n",
    "    x2 = layers.LeakyReLU()(x1)\n",
    "    x2 = layers.Dense(units=x2.shape[-1])(x2)\n",
    "    x2 = layers.Dropout(dropout_rate)(x2)\n",
    "    transformer_features = layers.Add()([x1, x2])\n",
    "    transformer_features = layers.LayerNormalization()(transformer_features)\n",
    "    features = layers.Flatten()(transformer_features)\n",
    "\n",
    "    # Included the other features.\n",
    "    if other_features is not None:\n",
    "        features = layers.concatenate(\n",
    "            [features, layers.Reshape([other_features.shape[-1]])(other_features)]\n",
    "        )\n",
    "\n",
    "    # Fully-connected layers.\n",
    "    for num_units in hidden_units:\n",
    "        features = layers.Dense(num_units)(features)\n",
    "        features = layers.BatchNormalization()(features)\n",
    "        features = layers.LeakyReLU()(features)\n",
    "        features = layers.Dropout(dropout_rate)(features)\n",
    "\n",
    "    outputs = layers.Dense(units=1)(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "M = create_model()\n",
    "\n",
    "################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_id(l, d):\n",
    "    s=\"\"\n",
    "    for i in l:\n",
    "        s+=d[i]+\",\"\n",
    "    return s[:-1]\n",
    "\n",
    "def tune_HL(a):\n",
    "    a=str(a)\n",
    "    a=a.replace(\", \", \",\")\n",
    "    a=a.replace(\"[\", \"\")\n",
    "    a=a.replace(\"]\", \"\")\n",
    "    return a\n",
    "\n",
    "\n",
    "#########   Input  ##############\n",
    "wh = 29606863; \n",
    "mat = [57005, 3372, 6013, 9974]; \n",
    "hl = [25.2,14.4,6.4,8.0]\n",
    "\n",
    "##note: un-comment this if using log model\n",
    "# hl = np.log(1+np.array(hl))\n",
    "\n",
    "foo = {\n",
    "    \"Wholesaler\": [\"WH_1\"],\n",
    "    \"material\": [transform_to_id(mat, mt_dict)],\n",
    "    \"hl_seq\": [tune_HL(hl)],\n",
    "    \"group\": [wh_to_group[wh]],\n",
    "    \"mid\": [wh_to_mid[wh]]\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(foo).to_csv(\"foo.csv\", index=False, sep=\"|\", header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo_dataset = get_dataset_from_csv(\"foo.csv\", shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.7742944]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.load_weights(\"HL_model_log\")\n",
    "(M.predict(foo_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
